{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX949eBE6OyF",
        "outputId": "ae8837f3-59d9-4ada-fa10-007376089909"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['coat']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "zip_file_path = \"/content/coat.zip\"  # Update with actual filename\n",
        "extract_path = \"/content/coat/\"\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List extracted files\n",
        "os.listdir(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ydw1plNg0PuB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load COAT dataset (modify paths as needed)\n",
        "def load_coat_data():\n",
        "    # Load train (biased) and test (unbiased) data\n",
        "    train_data = np.loadtxt(\"/content/coat/coat/train.ascii\")  # Shape: [users, items]\n",
        "    test_data = np.loadtxt(\"/content/coat/coat/test.ascii\")    # Shape: [users, items]\n",
        "\n",
        "    # Binarize ratings (ratings <4 → 0, else 1)\n",
        "    train_data = np.where(train_data < 4, 0, 1)\n",
        "    test_data = np.where(test_data < 4, 0, 1)\n",
        "\n",
        "    # Split 5% of test data as validation for methods needing unbiased data\n",
        "    np.random.seed(42)\n",
        "    unbiased_indices = np.where(test_data != -1)  # Assuming missing entries are marked as -1\n",
        "    val_indices = np.random.choice(len(unbiased_indices[0]),\n",
        "                                   size=int(0.05 * len(unbiased_indices[0])),\n",
        "                                   replace=False)\n",
        "    val_mask = np.zeros_like(test_data, dtype=bool)\n",
        "    val_mask[unbiased_indices[0][val_indices], unbiased_indices[1][val_indices]] = True\n",
        "    test_data[val_mask] = -1  # Mask validation entries in test\n",
        "\n",
        "    return train_data, test_data, val_mask\n",
        "\n",
        "train_data, test_data, val_mask = load_coat_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw8-eNAhX4sB"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyhLmCrA1gKr",
        "outputId": "9d94ab89-d9e7-4517-c47c-2457388368cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 4.3527\n",
            "Epoch 2/10, Loss: 3.7052\n",
            "Epoch 3/10, Loss: 3.1161\n",
            "Epoch 4/10, Loss: 2.5341\n",
            "Epoch 5/10, Loss: 1.9639\n",
            "Epoch 6/10, Loss: 1.4062\n",
            "Epoch 7/10, Loss: 0.9163\n",
            "Epoch 8/10, Loss: 0.5561\n",
            "Epoch 9/10, Loss: 0.3500\n",
            "Epoch 10/10, Loss: 0.2413\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MF(\n",
              "  (user_emb): Embedding(290, 64)\n",
              "  (item_emb): Embedding(300, 64)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MF(nn.Module):\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, latent_dim)\n",
        "        self.item_emb = nn.Embedding(n_items, latent_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_emb(user)\n",
        "        item_emb = self.item_emb(item)\n",
        "        pred = torch.sum(user_emb * item_emb, dim=1)\n",
        "        return self.sigmoid(pred)\n",
        "\n",
        "# Training loop for MF (biased)\n",
        "def train_mf(model, train_data, epochs=10, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    users, items = np.where(train_data != -1)\n",
        "    ratings = train_data[users, items]\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        torch.LongTensor(users),\n",
        "        torch.LongTensor(items),\n",
        "        torch.FloatTensor(ratings)\n",
        "    )\n",
        "    loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    # List to store losses\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for u, i, r in loader:\n",
        "            pred = model(u, i)\n",
        "            loss = criterion(pred, r)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "       # Calculate average epoch loss\n",
        "        avg_loss = epoch_loss / len(loader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize and train MF\n",
        "n_users, n_items = train_data.shape\n",
        "mf_model = MF(n_users, n_items, latent_dim=64)\n",
        "train_mf(mf_model, train_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjocZw3jI5Dr",
        "outputId": "6eb6c064-9113-4afc-deba-ed648ad7335e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 7.4812\n",
            "Epoch 2/10, Loss: 4.8321\n",
            "Epoch 3/10, Loss: 3.5211\n",
            "Epoch 4/10, Loss: 2.5824\n",
            "Epoch 5/10, Loss: 1.7830\n",
            "Epoch 6/10, Loss: 1.1505\n",
            "Epoch 7/10, Loss: 0.7108\n",
            "Epoch 8/10, Loss: 0.4510\n",
            "Epoch 9/10, Loss: 0.3124\n",
            "Epoch 10/10, Loss: 0.2376\n"
          ]
        }
      ],
      "source": [
        "class IPS(nn.Module):\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, latent_dim)\n",
        "        self.item_emb = nn.Embedding(n_items, latent_dim)\n",
        "        self.propensity = nn.Sequential(\n",
        "            nn.Linear(latent_dim * 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_emb(user)\n",
        "        item_emb = self.item_emb(item)\n",
        "        pred = torch.sum(user_emb * item_emb, dim=1)\n",
        "        return self.sigmoid(pred)\n",
        "\n",
        "    def train_ips(self, train_data, epochs=10, lr=0.001, eps=1e-8):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        criterion = nn.BCELoss(reduction='none')  # Use reduction='none' to compute δ_{u,i} per sample\n",
        "\n",
        "        # Create dataset (include all user-item pairs in D)\n",
        "        users_all, items_all = np.indices(train_data.shape).reshape(2, -1)\n",
        "        ratings_all = train_data.ravel()  # Flatten the matrix\n",
        "        o = (ratings_all != -1).astype(float)  # Observation indicator (1 if observed, 0 otherwise)\n",
        "        dataset = TensorDataset(\n",
        "            torch.LongTensor(users_all),\n",
        "            torch.LongTensor(items_all),\n",
        "            torch.FloatTensor(ratings_all),\n",
        "            torch.FloatTensor(o)\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for u, i, r, o_batch in loader:\n",
        "                # Compute propensity scores p̂_{u,i}\n",
        "                user_emb = self.user_emb(u)\n",
        "                item_emb = self.item_emb(i)\n",
        "                prop_input = torch.cat([user_emb, item_emb], dim=1)\n",
        "                p_hat = torch.clamp(self.propensity(prop_input).squeeze(), min=eps, max=1-eps)\n",
        "\n",
        "                # Compute prediction error δ_{u,i}\n",
        "                pred = self(u, i)\n",
        "                delta = criterion(pred, r)  # δ_{u,i} = BCE(r_{u,i}, r̂_{u,i})\n",
        "\n",
        "                # Compute IPS loss: (o_{u,i} * δ_{u,i}) / p̂_{u,i}\n",
        "                loss = (o_batch * delta / p_hat).mean()\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(loader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Initialize and train IPS model\n",
        "ips_model = IPS(n_users=290, n_items=300, latent_dim=64)\n",
        "ips_model.train_ips(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Govk9BUE3uM",
        "outputId": "3e93aac3-2b08-46b5-d1bb-05754c913059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 4.6169\n",
            "Epoch 2/10, Loss: 2.9075\n",
            "Epoch 3/10, Loss: 2.1495\n",
            "Epoch 4/10, Loss: 1.5732\n",
            "Epoch 5/10, Loss: 1.0609\n",
            "Epoch 6/10, Loss: 0.6699\n",
            "Epoch 7/10, Loss: 0.4265\n",
            "Epoch 8/10, Loss: 0.2948\n",
            "Epoch 9/10, Loss: 0.2259\n",
            "Epoch 10/10, Loss: 0.1866\n"
          ]
        }
      ],
      "source": [
        "class DR(IPS):\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__(n_users, n_items, latent_dim)\n",
        "        # Imputation model for δ̂_u,i\n",
        "        self.imputation = nn.Sequential(\n",
        "            nn.Linear(latent_dim * 2, 1),  # Input: user + item embeddings\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def compute_dr_loss(self, u, i, r, eps=1e-8):\n",
        "        # Get embeddings\n",
        "        user_emb = self.user_emb(u)\n",
        "        item_emb = self.item_emb(i)\n",
        "        prop_input = torch.cat([user_emb, item_emb], dim=1)\n",
        "\n",
        "        # Predicted conversion rate (CVR)\n",
        "        pred = self(u, i)  # r̂_u,i\n",
        "\n",
        "        # Compute true loss δ_u,i (BCE)\n",
        "        delta = - (r * torch.log(pred + eps) + (1 - r) * torch.log(1 - pred + eps))\n",
        "\n",
        "        # Compute imputed error δ̂_u,i\n",
        "        delta_hat = self.imputation(prop_input).squeeze()\n",
        "\n",
        "        # Compute propensity score p̂_u,i (clamped to avoid division by 0)\n",
        "        p_hat = torch.clamp(self.propensity(prop_input).squeeze(), min=eps, max=1-eps)\n",
        "\n",
        "        # Observation indicator o_u,i (1 for observed interactions)\n",
        "        o = (r != -1).float()  # Assuming unobserved entries are marked with -1\n",
        "\n",
        "        # DR loss component\n",
        "        dr_term = delta_hat + (o * (delta - delta_hat)) / p_hat\n",
        "\n",
        "        return dr_term.mean()  # Average over batch\n",
        "\n",
        "    def train_dr(self, train_data, epochs=10, lr=0.001, eps=1e-8):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        losses = []\n",
        "\n",
        "        # Prepare dataset\n",
        "        users, items = np.where(train_data != -1)\n",
        "        ratings = train_data[users, items]\n",
        "        dataset = torch.utils.data.TensorDataset(\n",
        "            torch.LongTensor(users),\n",
        "            torch.LongTensor(items),\n",
        "            torch.FloatTensor(ratings)\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for u, i, r in loader:\n",
        "                # Compute DR loss\n",
        "                loss = self.compute_dr_loss(u, i, r, eps)\n",
        "\n",
        "                # Backpropagation with gradient clipping\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(loader)\n",
        "            losses.append(avg_loss)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return self, losses\n",
        "# Initialize and train the DR model\n",
        "dr_model = DR(n_users, n_items, latent_dim=64)\n",
        "trained_model, training_losses = dr_model.train_dr(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DBpQQZDKWr2",
        "outputId": "1af951f6-cbdb-4e66-ff72-d2bca98d6313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 11.4928\n",
            "Epoch 2/10, Loss: 10.2022\n",
            "Epoch 3/10, Loss: 9.4215\n",
            "Epoch 4/10, Loss: 8.7182\n",
            "Epoch 5/10, Loss: 8.0937\n",
            "Epoch 6/10, Loss: 7.5025\n",
            "Epoch 7/10, Loss: 6.9875\n",
            "Epoch 8/10, Loss: 6.5269\n",
            "Epoch 9/10, Loss: 6.1132\n",
            "Epoch 10/10, Loss: 5.7415\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ==================== 1. Define Base Components ====================\n",
        "class ResidualLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.residual(x)\n",
        "\n",
        "class IPS(nn.Module):\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, latent_dim)\n",
        "        self.item_emb = nn.Embedding(n_items, latent_dim)\n",
        "        self.propensity = nn.Sequential(\n",
        "            nn.Linear(latent_dim*2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_emb(user)\n",
        "        item_emb = self.item_emb(item)\n",
        "        return torch.sigmoid(torch.sum(user_emb * item_emb, dim=1))\n",
        "\n",
        "# ==================== 2. Implement Res-IPS ====================\n",
        "class ResIPS(IPS):\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__(n_users, n_items, latent_dim)\n",
        "        # Residual networks\n",
        "        self.residual_p = nn.Sequential(\n",
        "            ResidualLayer(latent_dim*2),\n",
        "            nn.Linear(latent_dim*2, 1)\n",
        "        )\n",
        "        self.residual_delta = nn.Sequential(\n",
        "            ResidualLayer(latent_dim*2),\n",
        "            nn.Linear(latent_dim*2, 1)\n",
        "        )\n",
        "        # Imputation model\n",
        "        self.imputation = nn.Sequential(\n",
        "            nn.Linear(latent_dim*2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _prepare_data(self, train_data, val_data):\n",
        "        \"\"\"Create data loaders for training and validation\"\"\"\n",
        "        # Training data (biased)\n",
        "        train_users, train_items = np.where(train_data != -1)\n",
        "        train_ratings = train_data[train_users, train_items]\n",
        "        train_dataset = TensorDataset(\n",
        "            torch.LongTensor(train_users),\n",
        "            torch.LongTensor(train_items),\n",
        "            torch.FloatTensor(train_ratings)\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        # Validation data (unbiased)\n",
        "        val_users, val_items = np.where(val_data != -1)\n",
        "        val_ratings = val_data[val_users, val_items]\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.LongTensor(val_users),\n",
        "            torch.LongTensor(val_items),\n",
        "            torch.FloatTensor(val_ratings)\n",
        "        )\n",
        "        val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def compute_loss(self, u, i, r, vu, vi, vr, alpha=0.5, beta=1.0, gamma=0.01, eps=1e-8):\n",
        "        \"\"\"Calculate complete Res-IPS loss\"\"\"\n",
        "        # Feature concatenation\n",
        "        user_emb = self.user_emb(u)\n",
        "        item_emb = self.item_emb(i)\n",
        "        phi = torch.cat([user_emb, item_emb], dim=1)\n",
        "\n",
        "        # Propensity calibration\n",
        "        p_nominal = torch.clamp(self.propensity(phi).squeeze(), min=eps, max=1-eps)\n",
        "        p_res = self.residual_p(phi).squeeze()\n",
        "        p_tilde = torch.sigmoid(torch.logit(p_nominal) + p_res).clamp(eps, 1-eps)\n",
        "\n",
        "        # Imputation calibration\n",
        "        delta_nominal = self.imputation(phi).squeeze()\n",
        "        delta_res = self.residual_delta(phi).squeeze()\n",
        "        delta_tilde = torch.sigmoid(torch.logit(delta_nominal) + delta_res).clamp(eps, 1-eps)\n",
        "\n",
        "        # Loss components\n",
        "        L_ctr = F.binary_cross_entropy(p_nominal, r)\n",
        "        L_imp = F.mse_loss(delta_nominal, (r - self(u,i)).detach())\n",
        "        L_cvr_b = (F.binary_cross_entropy(self(u,i), r, reduction='none') / p_tilde).mean()\n",
        "        L_ctcvr_b = F.binary_cross_entropy(p_tilde * self(u,i), r)\n",
        "        L_cvr_u = F.binary_cross_entropy(self(vu,vi), vr)\n",
        "        L_consistency = F.mse_loss(p_nominal, p_tilde) + F.mse_loss(delta_nominal, delta_tilde)\n",
        "\n",
        "        return L_ctr + alpha*L_imp + beta*(L_cvr_b + L_ctcvr_b + L_cvr_u) + gamma*L_consistency\n",
        "\n",
        "    def train_resips(self, train_data, val_data, epochs=10, lr=0.001, **kwargs):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        train_loader, val_loader = self._prepare_data(train_data, val_data)\n",
        "        losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for (u,i,r), (vu,vi,vr) in zip(train_loader, val_loader):\n",
        "                loss = self.compute_loss(u,i,r, vu,vi,vr, **kwargs)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss/len(train_loader)\n",
        "            losses.append(avg_loss)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return losses\n",
        "\n",
        "# ==================== 3. Initialize and Train ====================\n",
        "# Sample data (replace with actual data)\n",
        "n_users, n_items = 290, 300\n",
        "train_data = np.random.choice([0,1,-1], size=(n_users, n_items), p=[0.3,0.2,0.5])\n",
        "test_data = np.random.choice([0,1], size=(n_users, n_items))\n",
        "\n",
        "# Create and train model\n",
        "resips_model = ResIPS(n_users, n_items, latent_dim=64)\n",
        "loss_history = resips_model.train_resips(\n",
        "    train_data,\n",
        "    val_data=test_data,\n",
        "    alpha=0.5,\n",
        "    beta=1.0,\n",
        "    gamma=0.01,\n",
        "    lr=0.001,\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeZYMXXxMqG7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class ResidualLayer(nn.Module):\n",
        "    \"\"\"Residual network for propensity/imputation calibration\"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.residual(x)\n",
        "\n",
        "class IPS(nn.Module):\n",
        "    \"\"\"Base IPS model with propensity estimation\"\"\"\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, latent_dim)\n",
        "        self.item_emb = nn.Embedding(n_items, latent_dim)\n",
        "        self.propensity = nn.Sequential(\n",
        "            nn.Linear(latent_dim*2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_emb(user)\n",
        "        item_emb = self.item_emb(item)\n",
        "        return torch.sigmoid((user_emb * item_emb).sum(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq-DYUBUMqDd"
      },
      "outputs": [],
      "source": [
        "class ResIPS(IPS):\n",
        "    \"\"\"Res-IPS with calibrated propensities and imputations\"\"\"\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__(n_users, n_items, latent_dim)\n",
        "        # Residual networks\n",
        "        self.residual_p = nn.Sequential(\n",
        "            ResidualLayer(latent_dim*2),\n",
        "            nn.Linear(latent_dim*2, 1)\n",
        "        )\n",
        "        self.residual_d = nn.Sequential(\n",
        "            ResidualLayer(latent_dim*2),\n",
        "            nn.Linear(latent_dim*2, 1)\n",
        "        )\n",
        "        # Imputation model\n",
        "        self.imputation = nn.Sequential(\n",
        "            nn.Linear(latent_dim*2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def _prepare_data(self, train_data, val_data):\n",
        "        \"\"\"Create data loaders for training (biased) and validation (unbiased)\"\"\"\n",
        "        # Training data (observed interactions)\n",
        "        train_users, train_items = np.where(train_data != -1)\n",
        "        train_ratings = train_data[train_users, train_items]\n",
        "        train_dataset = TensorDataset(\n",
        "            torch.LongTensor(train_users),\n",
        "            torch.LongTensor(train_items),\n",
        "            torch.FloatTensor(train_ratings)\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        # Validation data (unbiased)\n",
        "        val_users, val_items = np.where(val_data != -1)\n",
        "        val_ratings = val_data[val_users, val_items]\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.LongTensor(val_users),\n",
        "            torch.LongTensor(val_items),\n",
        "            torch.FloatTensor(val_ratings)\n",
        "        )\n",
        "        val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def compute_loss(self, u, i, r, vu, vi, vr, alpha=0.5, beta=1.0, gamma=0.01, eps=1e-8):\n",
        "        \"\"\"Res-IPS Loss Calculation (from paper)\"\"\"\n",
        "        # Feature embeddings\n",
        "        user_emb = self.user_emb(u)\n",
        "        item_emb = self.item_emb(i)\n",
        "        phi = torch.cat([user_emb, item_emb], dim=1)\n",
        "\n",
        "        # Propensity calibration\n",
        "        p_nom = torch.clamp(self.propensity(phi).squeeze(), min=eps, max=1-eps)\n",
        "        p_res = self.residual_p(phi).squeeze()\n",
        "        p_tilde = torch.sigmoid(torch.logit(p_nom) + p_res).clamp(eps, 1-eps)\n",
        "\n",
        "        # Imputation calibration\n",
        "        delta_nom = self.imputation(phi).squeeze()\n",
        "        delta_res = self.residual_d(phi).squeeze()\n",
        "        delta_tilde = torch.sigmoid(torch.logit(delta_nom) + delta_res).clamp(eps, 1-eps)\n",
        "\n",
        "        # Loss components\n",
        "        pred = self(u, i)\n",
        "        L_ctr = F.binary_cross_entropy(p_nom, r)  # CTR loss\n",
        "        L_imp = F.mse_loss(delta_nom, (r - pred).detach())  # Imputation loss\n",
        "        L_cvr_b = (F.binary_cross_entropy(pred, r, reduction='none') / p_tilde).mean()  # IPS term\n",
        "        L_ctcvr_b = F.binary_cross_entropy(p_tilde * pred, r)  # CTCVR loss\n",
        "        L_cvr_u = F.binary_cross_entropy(self(vu, vi), vr)  # Unbiased loss\n",
        "        L_cons = F.mse_loss(p_nom, p_tilde) + F.mse_loss(delta_nom, delta_tilde)  # Consistency\n",
        "\n",
        "        return L_ctr + alpha*L_imp + beta*(L_cvr_b + L_ctcvr_b + L_cvr_u) + gamma*L_cons\n",
        "\n",
        "    def train_model(self, train_data, val_data, epochs=10, lr=0.001, **kwargs):\n",
        "        \"\"\"Training loop for Res-IPS\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        train_loader, val_loader = self._prepare_data(train_data, val_data)\n",
        "        losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for (u, i, r), (vu, vi, vr) in zip(train_loader, val_loader):\n",
        "                loss = self.compute_loss(u, i, r, vu, vi, vr, **kwargs)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            losses.append(avg_loss)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7nZW9hbMqA0"
      },
      "outputs": [],
      "source": [
        "class ResDR(ResIPS):\n",
        "    \"\"\"Res-DR extends Res-IPS with Doubly Robust term\"\"\"\n",
        "    def compute_loss(self, u, i, r, vu, vi, vr, alpha=0.5, beta=1.0, gamma=0.01, eps=1e-8):\n",
        "        \"\"\"Res-DR Loss Calculation (from paper)\"\"\"\n",
        "        # Reuse Res-IPS components\n",
        "        user_emb = self.user_emb(u)\n",
        "        item_emb = self.item_emb(i)\n",
        "        phi = torch.cat([user_emb, item_emb], dim=1)\n",
        "\n",
        "        # Calibrated terms\n",
        "        p_nom = torch.clamp(self.propensity(phi).squeeze(), min=eps, max=1-eps)\n",
        "        p_res = self.residual_p(phi).squeeze()\n",
        "        p_tilde = torch.sigmoid(torch.logit(p_nom) + p_res).clamp(eps, 1-eps)\n",
        "\n",
        "        delta_nom = self.imputation(phi).squeeze()\n",
        "        delta_res = self.residual_d(phi).squeeze()\n",
        "        delta_tilde = torch.sigmoid(torch.logit(delta_nom) + delta_res).clamp(eps, 1-eps)\n",
        "\n",
        "        # Compute base Res-IPS loss\n",
        "        pred = self(u, i)\n",
        "        L_ctr = F.binary_cross_entropy(p_nom, r)\n",
        "        L_imp = F.mse_loss(delta_nom, (r - pred).detach())\n",
        "        L_ctcvr_b = F.binary_cross_entropy(p_tilde * pred, r)\n",
        "        L_cvr_u = F.binary_cross_entropy(self(vu, vi), vr)\n",
        "        L_cons = F.mse_loss(p_nom, p_tilde) + F.mse_loss(delta_nom, delta_tilde)\n",
        "\n",
        "        # Doubly Robust term (replace IPS with DR)\n",
        "        delta = F.binary_cross_entropy(pred, r, reduction='none')\n",
        "        L_dr_b = (delta_tilde + (delta - delta_tilde) / p_tilde).mean()\n",
        "\n",
        "        return L_ctr + alpha*L_imp + beta*(L_dr_b + L_ctcvr_b + L_cvr_u) + gamma*L_cons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1oB_IHFMp-Q"
      },
      "outputs": [],
      "source": [
        "# Initialize data\n",
        "n_users, n_items = 290, 300\n",
        "train_data = np.random.choice([0, 1, -1], size=(n_users, n_items), p=[0.3, 0.2, 0.5])  # Biased\n",
        "test_data = np.random.choice([0, 1], size=(n_users, n_items))  # Unbiased\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmSf246rLzUg",
        "outputId": "9a19d1be-538f-48da-b3a8-79abe5ca72e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 11.8021\n",
            "Epoch 2/10, Loss: 10.3168\n",
            "Epoch 3/10, Loss: 9.5094\n",
            "Epoch 4/10, Loss: 8.7956\n",
            "Epoch 5/10, Loss: 8.1624\n",
            "Epoch 6/10, Loss: 7.5689\n",
            "Epoch 7/10, Loss: 7.0252\n",
            "Epoch 8/10, Loss: 6.5571\n",
            "Epoch 9/10, Loss: 6.1221\n",
            "Epoch 10/10, Loss: 5.7382\n"
          ]
        }
      ],
      "source": [
        "#Train Res-IPS\n",
        "resips = ResIPS(n_users, n_items, latent_dim=64)\n",
        "resips_loss = resips.train_model(\n",
        "    train_data, test_data,\n",
        "    alpha=0.5, beta=1.0, gamma=0.01,\n",
        "    lr=0.001, epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsRjaCh1Jrgd",
        "outputId": "0de695b2-477a-45f6-eb4d-eacc46646334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 11.7354\n",
            "Epoch 2/10, Loss: 10.4335\n",
            "Epoch 3/10, Loss: 9.6362\n",
            "Epoch 4/10, Loss: 8.8784\n",
            "Epoch 5/10, Loss: 8.2204\n",
            "Epoch 6/10, Loss: 7.6223\n",
            "Epoch 7/10, Loss: 7.0855\n",
            "Epoch 8/10, Loss: 6.5779\n",
            "Epoch 9/10, Loss: 6.1300\n",
            "Epoch 10/10, Loss: 5.7449\n"
          ]
        }
      ],
      "source": [
        "# Train Res-DR\n",
        "resdr = ResDR(n_users, n_items, latent_dim=64)\n",
        "resdr_loss = resdr.train_model(\n",
        "    train_data, test_data,\n",
        "    alpha=0.5, beta=1.0, gamma=0.01,\n",
        "    lr=0.001, epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtdMLQTjOms4"
      },
      "outputs": [],
      "source": [
        "# ==================== 1. Revised Model Implementations ====================\n",
        "class MF(nn.Module):\n",
        "    \"\"\"Matrix Factorization (Base Model)\"\"\"\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, latent_dim)\n",
        "        self.item_emb = nn.Embedding(n_items, latent_dim)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_emb(user)\n",
        "        item_emb = self.item_emb(item)\n",
        "        return torch.sigmoid((user_emb * item_emb).sum(dim=1))\n",
        "\n",
        "    def train_model(self, train_data, epochs=10, lr=0.001):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Prepare data\n",
        "        users, items = np.where(train_data != -1)\n",
        "        ratings = train_data[users, items]\n",
        "        dataset = TensorDataset(\n",
        "            torch.LongTensor(users),\n",
        "            torch.LongTensor(items),\n",
        "            torch.FloatTensor(ratings)\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for u, i, r in loader:\n",
        "                pred = self(u, i)\n",
        "                loss = criterion(pred, r)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "class IPS(MF):\n",
        "    \"\"\"Inverse Propensity Scoring\"\"\"\n",
        "    def __init__(self, n_users, n_items, latent_dim):\n",
        "        super().__init__(n_users, n_items, latent_dim)\n",
        "        self.propensity = nn.Sequential(\n",
        "            nn.Linear(latent_dim*2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def train_model(self, train_data, epochs=10, lr=0.001):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        # Prepare data\n",
        "        users, items = np.where(train_data != -1)\n",
        "        ratings = train_data[users, items]\n",
        "        dataset = TensorDataset(\n",
        "            torch.LongTensor(users),\n",
        "            torch.LongTensor(items),\n",
        "            torch.FloatTensor(ratings)\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for u, i, r in loader:\n",
        "                # Get embeddings\n",
        "                user_emb = self.user_emb(u)\n",
        "                item_emb = self.item_emb(i)\n",
        "                prop_input = torch.cat([user_emb, item_emb], dim=1)\n",
        "\n",
        "                # Compute loss\n",
        "                p = torch.clamp(self.propensity(prop_input).squeeze(), 1e-8, 1-1e-8)\n",
        "                pred = self(u, i)\n",
        "                loss = (F.binary_cross_entropy(pred, r, reduction='none') / p).mean()\n",
        "\n",
        "                # Update\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "# Keep previous implementations for DR, ResIPS, ResDR unchanged\n",
        "# [Use the DR, ResIPS, ResDR implementations from previous answer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xFP9-lnOqUk",
        "outputId": "d222e3ad-7c5d-46b1-89fa-f67284f91362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training MF ===\n",
            "\n",
            "=== Training IPS ===\n",
            "\n",
            "=== Training DR ===\n",
            "\n",
            "=== Training Res-IPS ===\n",
            "Epoch 1/10, Loss: 11.4889\n",
            "Epoch 2/10, Loss: 10.2679\n",
            "Epoch 3/10, Loss: 9.4557\n",
            "Epoch 4/10, Loss: 8.7666\n",
            "Epoch 5/10, Loss: 8.1238\n",
            "Epoch 6/10, Loss: 7.5225\n",
            "Epoch 7/10, Loss: 7.0109\n",
            "Epoch 8/10, Loss: 6.5247\n",
            "Epoch 9/10, Loss: 6.1140\n",
            "Epoch 10/10, Loss: 5.7260\n",
            "\n",
            "=== Training Res-DR ===\n",
            "Epoch 1/10, Loss: 11.5682\n",
            "Epoch 2/10, Loss: 10.2791\n",
            "Epoch 3/10, Loss: 9.4699\n",
            "Epoch 4/10, Loss: 8.7692\n",
            "Epoch 5/10, Loss: 8.1088\n",
            "Epoch 6/10, Loss: 7.5245\n",
            "Epoch 7/10, Loss: 7.0002\n",
            "Epoch 8/10, Loss: 6.5252\n",
            "Epoch 9/10, Loss: 6.0790\n",
            "Epoch 10/10, Loss: 5.6737\n",
            "\n",
            "=== Final Results ===\n",
            "Model      AUC      Recall@5   NDCG@5    \n",
            "MF         0.499    0.49       0.524     \n",
            "IPS        0.501    0.471      0.472     \n",
            "DR         0.499    0.497      0.483     \n",
            "Res-IPS    0.527    0.511      0.517     \n",
            "Res-DR     0.524    0.514      0.556     \n"
          ]
        }
      ],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    \"MF\": MF(290, 300, 64),\n",
        "    \"IPS\": IPS(290, 300, 64),\n",
        "    \"DR\": DR(290, 300, 64),\n",
        "    \"Res-IPS\": ResIPS(290, 300, 64),\n",
        "    \"Res-DR\": ResDR(290, 300, 64)\n",
        "}\n",
        "\n",
        "# Train and evaluate\n",
        "results = train_and_evaluate(\n",
        "    models,\n",
        "    train_data,\n",
        "    test_data,\n",
        "    epochs=10,\n",
        "    k=5\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Final Results ===\")\n",
        "print(f\"{'Model':<10} {'AUC':<8} {'Recall@5':<10} {'NDCG@5':<10}\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"{model:<10} {metrics['AUC']:<8} {metrics['Recall@5']:<10} {metrics['NDCG@5']:<10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aE7GstQJrbx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhCXlcS4JrZN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKZDhw8NJrWk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}